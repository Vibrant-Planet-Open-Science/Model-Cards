# VibrantForests: Model Card
Jump to section:

- [Model details](#model-details)
- [Intended use](#intended-use)
- [Factors](#factors)
- [Metrics](#metrics)
- [Evaluation data](#evaluation-data)
- [Training data](#training-data)
- [Training process](#training-process)
- [Quantitative analyses](#quantitative-analyses)
- [Ethical considerations](#ethical-considerations)
- [Caveats and recommendations](#caveats-and-recommendations)
- [Acknowledgements](#acknowledgements)

## Model details
VibrantForests is a multitarget regression model developed by [Vibrant Planet](https://www.vibrantplanet.net/) to estimate five fundamental forest structure attributes at 10-meter (10m) resolution[^1] from satellite imagery: 
* **AGB**: aboveground live tree biomass [Mg/ha]
* **QMD**: quadratic mean diameter [cm]
* **BA**: basal area of live trees [m²/ha]
* **Cover**: canopy cover [%]
* **Height**: canopy height [cm]

The additional metrics trees per hectare (**TPH**) and Stand Density Index (**SDI**) are then derived deterministically.

VibrantForests was trained using a context window of 256 x 256 pixels, enabling the model to capture spatial patterns from local- to stand-scales. Inference was executed using sliding overlapping windows, which were subsequently mosaicked together to produce seamless raster outputs.

### Contributors
Luke Zachmann, David Diaz, Tony Chang, Nathan Rutenbeck, Vincent Landau, Katharyn Duffy, Andreas Gros, Kiarie Ndegwa, Scott Conway, Guy Bayes

### Model date
November 2025

### Model version
1.1.1

### Model type
VibrantForests uses a Feature Pyramid Network (FPN) architecture built on VibrantMAE, a Vision Transformer-based Masked AutoEncoder (MAE). The VibrantMAE encoder serves as a frozen feature extractor, producing multi-scale feature representations extracted at multiple transformer depths. These features are then processed through the FPN decoder and a pixelwise regression head to predict the five forest structure attributes.

## Intended use

### Primary intended uses
This model was designed to support forest mapping and monitoring with direct applications in a decision support system for wildfire risk assessment and land management planning. It is intended to produce estimates with annual update frequency at sufficient resolution to enable planning and prioritization activities at spatial scales ranging from individual forest stands up to large landscapes. The model was developed with an initial geographic focus on the contiguous United States (CONUS) following modeling approaches that can be readily extended to other forested regions. 

VibrantForests was also developed to provide a springboard from which additional forest attributes could be derived (e.g., relative density, forest product volumes, etc.). Together, these raw and derived attributes are intended to enable assessments of forest management impacts on the land including evaluations of the efficacy of management interventions at landscape scales at achieving objectives like reducing the extent and severity of wildfires and associated risks to communities.

### Primary intended users
The intended direct users of VibrantForests outputs are natural resource managers and other professionals leading wildfire risk assessments, community wildfire protection planning, and forest restoration planning through the [Vibrant Planet Platform](https://www.vibrantplanet.net/platform). These use cases generally involve access to zonal summaries of forest and other land attributes at the scale of management units rather than accessing raster data directly. 

Selected outputs of VibrantForests as well as derived attributes using additional models (SDI, relative density, and merchantable timber volume) are being produced for inclusion in the [Forest Innovation Platform](https://forestinnovationplatform.org/), a web application under development by American Forests intended to be used by analysts supporting rapid assessments of forest vulnerability and climate adaptation planning.

The AGB rasters generated by VibrantForests are expected to be directly useful for project developers and verification bodies concerned with carbon monitoring and reporting for forest restoration and wildfire mitigation programs. We also anticipate the combination of carbon, timber, and other forest impact metrics to be used by wood procurement teams, and sawmills and other forest products facilities interested in quantifying and reporting the embodied impacts of timber utilized from their supply areas.

### Out-of-scope use cases
Although we anticipate uses of VibrantForests raster outputs by forest researchers and analysts in the public and private sectors for regional  analyses (e.g., generating population estimates like means and standard errors across large areas), we have not substantially altered model development based on these potential use cases.

VibrantForests was also not intended to support individual tree detection or attribution. The 10m resolution of the model is well-suited for representing forest attributes on an areal basis and not for individual tree detection and attribution.

## Factors
The key factors shaping the development and application of VibrantForests included the utilization of imagery from the Sentinel-2 satellite constellation and our choice to limit model inputs to features that can be extended globally. The use of Sentinel-2 imagery limits the applicability of VibrantForests back in time to 2018 based on availability of data for generating seasonal composites from Sentinel-2 imagery.  

Biogeographic factors that influence forest structure and composition were incorporated into the preparation of training data by fitting regionally-distinct allometric models and in the evaluation of models by calculating performance metrics in each ecoregion.

## Metrics
We evaluated model performance at plot scale using individual forest inventory plots, as well as at regional scales using moments and distributions of observations from National Forest Inventory data.

### Plot scale evaluation
Plot-level performance is reported using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), mean bias, R-squared (R²), and Pearson's R.  

Qualitative assessments were conducted through visual inspections of scatterplots of predicted versus observed values, and of density plots which overlay distributions of observed and predicted forest attributes. 

### Pixel-scale evaluation
Pixel-scale performance was quantified using the same metrics as plot-scale evaluations. 

### Region-scale evaluation
To quantitatively evaluate model bias at regional scales, we summarized predicted and FIA-observed values to the scale of 64,000-hectare (ha) hexagons. We compared observed and predicted hexagon-level means in a scatter plot as well as a chloropleth map to visualize spatial patterns in bias.

## Evaluation data

### Data for pixel-scale evaluation
To characterize model learning from training data, we utilized tiles from the test partition that were withheld from model training. Methods for generating the training tiles are described further below under [Training Data](#training-data). 

> [!CAUTION]
> The pixel-scale evaluations described in this model card do not characterize model performance against ground-truth. These evaluations only illustrate how well the model learned patterns in the training data. These results **should not** be misinterpreted as indications of how well the model performs at plot- or landscape-scales in the real world. We provided these pixel-scale evaluations following the pattern of other research groups to facilitate model comparison regarding how well different model architectures encode patterns from training data. 

To evaluate the quality of training tiles against ground-truth, we compared forest structure estimates from training tiles with independent forest inventory plots (described below) that were located within the footprints of these tiles.

### Data for plot-scale evaluation
Forest plot data collected by the Bureau of Land Management (BLM; Oregon), US Forest Service (USFS; Oregon & Washington), and Washington Department of Natural Resources (DNR) from 2010 through 2018 were compiled through a combination of direct correspondence and public records requests. The data include raw field measurements with plot locations, as well as tree species, diameter, height, and live/dead status among several other attributes. 

Plot sizes and sampling protocols varied among the agencies, as did the dates of field data collection. In general, BLM and USFS conducted regional field campaigns to correspond with the collection of aerial lidar over specific National Forests or BLM Districts. Washington DNR collected field plot data to correspond with statewide NAIP imagery collected in 2021 that was processed using photogrammetry to generate point clouds.

#### Motivation
These data are rare examples of confidently-located plot-level forest observations in managed forest landscapes that can be used as ground-truth. We have identified several other sources of plot- and stand-based measurements in managed forests, reserves, and research sites and will continue to collect and ingest additional field measurements over time to improve the diversity and comprehensiveness of datasets available for model evaluation. 

#### Preprocessing
Field plot measurements were compiled using the appropriate regional variant of the Forest Vegetation Simulator (FVS) to calculate dominant height, AGB, and many other attributes from the raw inventory data. We did not utilize canopy cover estimates generated by FVS, which have been shown to be systematically biased compared to field-based and remote sensing methods ([Gray et al., 2021](https://doi.org/10.1016/j.foreco.2021.119682)). Instead, we utilized lidar-derived estimates of canopy cover for each plot footprint from the most recent lidar collection available—if any—over each plot. 

To avoid introducing further error and bias, field plots were not grown forward with FVS. Following inspections of earlier growth-and-yield simulations for these plots, we noticed dozens of plots with increased bias and error related to significant mortality predicted by FVS, for example. We thus limited the use of FVS to calculate plot attributes at the time of measurement and accepted the time lag between field observations and the acquisition of imagery used in model predictions as a more consistent and more easily interpretable source of error than growth-and-yield modeling would introduce.

### Data for regional evaluation
We utilized National Forest Inventory measurements limited to publicly-released records in Forest Inventory & Analysis (FIA) databases. To facilitate comparison with other data providers, we included an evaluation of our forest structure predictions against the [Menlove and Healey (2020)](https://research.fs.usda.gov/treesearch/63383) AGB data layer where FIA observations from 2009-2019 were averaged to the scale of 64,000ha hexagons. 

Because Menlove and Healey (2020) only summarized AGB among our target variables, we queried the FIA database for the most recent 10 years of data available (2012-2022) to evaluate hexagon-level performance for all our targets. Hexagon assignments for each plot were based on the hexagon IDs linked to each plot in the FIA database. This query includes more recent FIA samples and additional hexagons that were not represented in the Menlove and Healey (2020) dataset.

#### Motivation
The FIA program provides a systematic and spatially balanced inventory of forests across CONUS. The geographic expanse and diversity, sample size, and revisit frequency provide a robust data source for model development and validation. However, due to the fact that the precise locations of FIA plots are not available for public use, we were compelled to limit our uses of FIA data to those that do not require precise location information. 

| ![Map showing the scale of FIA subplots against a 10m resolution prediction](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/forest_structure_regional_validation.png) |
| :-- |
| *Illustration of the scale of a cluster of FIA subplots against 10m resolution predictions. The larger map shows the geographic distribution of forested FIA subplots across CONUS from which our allometric model was derived.* |

#### Preprocessing
For evaluation data, we extracted forest structure attributes from FIA plots including both forested and non-forested plots. Forest structure attributes were set to zero where plots were recorded as non-sampled due to absence of forest cover. We aggregated wall-to-wall predictions from VibrantForests based on 2024 imagery to hexagon-level means, and made comparisons with FIA observations on an all-lands basis (without filtering by land cover type).

## Training data

### Features
VibrantForests was trained with Sentinel-2 satellite imagery as its sole input. Sentinel-2 surface reflectance values are summarized into a single June-July-August (JJA) composite for a given year and passed through the encoder of VibrantMAE, a geospatial foundation model trained as a Masked Auto-Encoder, which is employed in VibrantForests as a frozen feature extractor. VibrantForests internally combines VibrantMAE embeddings with the original Sentinel-2 composite to predict forest structure attributes. 

### Targets
Tiles of training targets were generated by applying an FIA-derived allometric model to lidar-derived rasters of canopy cover and canopy height. This approach enabled us to generate high-fidelity and richly-textured training tiles across large geographic extents at 10m resolution.

#### Allometric model development
Field data for all trees with diameter at breast height ≥ 1 inch were summarized for each FIA sample to fit an allometric model that predicted AGB, QMD, and BA as a function of field-measured canopy cover, maximum tree height, elevation, and ecoregion.

#### Lidar data processing and target generation
Lidar point clouds were sampled from the USGS 3DEP program, limited to acquisitions with a Quality Level rating of 2 or better. We processed the point clouds using [`PDAL`](https://pdal.org) to remove points classified as noise by the vendor and any points that were more than 2m below or 90m above the vendor-classified ground. Point clouds were processed into a 0.5m resolution Canopy Height Model (CHM) layer and a canopy cover layer at 10m resolution. Canopy cover was calculated as the percentage of returns above 2m divided by total number of returns. The FIA-derived allometric model was then applied to the lidar-derived cover and height layers to estimate the remaining target forest structure attributes to produce tiles for use in model training.

### Partitioning tiles for training, validation, and testing
All incoming raster and point cloud data were reprojected to EASE Grid 2.0 North (EPSG:6931) and tiled into 2,500m × 2,500m samples (with 30m padding). To prevent spatial data leakage between partitions, tiles were assigned to partitions based on their parent 15km × 15km grid cell. Grid cells were allocated to training (70%), validation (20%), or testing (10%) partitions using a deterministic hash-based method, ensuring consistent assignment across future model and data updates. A total of 73,920 256 × 256 pixel training samples were generated from 4,119 unique 15km grid cells.


| ![Map showing the geographic extent of training data across CONUS](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/forest_structure_training_footprint.png) |
| :--- |
| *The geographic extent of lidar-derived training data generated across CONUS. Given Vibrant Planet's initial concentration of decision support in western US forests, we utilized a larger number of lidar acquisitions that had already been processed in this area for previous modeling efforts. These western lidar acquisitions were complemented with a spatially balanced sample of new grid cells of lidar across CONUS selected to match the density of forested FIA plots in each ecoregion.* |

| ![Map showing the spatial distribution of training, validation, and test partitions](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/forest_structure_training_partitions.png) |
| :--- |
| *Illustration of spatial separation of training, validation, and test partitions. Tiles in the training partition are displayed in blue, validation partition in purple, and test partition in orange.* |

### Training process
We trained VibrantForests using [`PyTorch Lightning`](https://lightning.ai/docs/pytorch/stable/) for up to 150 epochs with early stopping based on monitoring validation loss (patience of 10 epochs). The model was trained as a quantile regression model, outputting predictions for five quantile levels (0.1, 0.3, 0.5, 0.7, 0.9) for each target. This approach allows the model to explicitly learn multiple points of the conditional distribution (10<sup>th</sup>, 30<sup>th</sup>, 50<sup>th</sup>, 70<sup>th</sup>, and 90<sup>th</sup> percentiles), providing built-in uncertainty quantification. Unlike standard regression which outputs only a single point estimate (typically the conditional mean), quantile regression captures the shape and spread of the distribution, which is particularly useful for the right-skewed distributions of our target variables.  

For optimization, we used the AdamW algorithm with a learning rate of 1e-4 and a step decay scheduler (step size of 10 epochs and gamma of 0.7), weight decay of 0.05, and betas of (0.9, 0.999). The loss was scaled based on the ratio of valid pixels to account for spatial variation in data availability.

### Post-processing of model outputs
VibrantForests outputs were generated initially as 256 x 256 pixel rasters. We employed a mosaicking method to eliminate edge effects (e.g., seams at tile edges) by executing inference using a sliding window and then calculated the final product as a weighted average of the overlapping tiles. In this version of the model, only the median prediction (q=0.5) was used to generate wall-to-wall mosaics.

| ![Forest structure inference using torch fold/unfold](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/forest_structure_inference.gif) |
| :--- |
| *Illustration of forest structure predictions with sliding and overlapping inference windows to generate mosaicked output for a single tile.* |

## Quantitative analyses
### Plot-scale evaluation
We found strong correspondence between observed and predicted values for AGB, Cover, Height, and BA with modest saturation effects appearing after the 75<sup>th</sup> or  90<sup>th</sup> percentile of observations. The density curves of predictions and observations located over these field plots demonstrated that VibrantForests generated predictions spanning the full range of observed values, including very high biomass and very tall canopy conditions.

| !["Overall performance of VibrantForests on independent field observations"](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/plot_level_evaluation_for_predictions.png) |
| :---- |
| *Overall performance of VibrantForests on independent field observations. The black points in each scatter plot show the average value predicted for each quantile of observations, spanning the 5<sup>th</sup> through 95<sup>th</sup> percentiles in steps of 5.* |

> [!NOTE]  
> The saturation effects reported here are comparable to those observed when forest structure models have been fit directly to lidar point cloud features. VibrantForests extends the predictive range with little-to-no-bias into larger and denser forest conditions than has typically been reported for models relying exclusively on passive remote sensing like surface reflectance from aerial and satellite imaging.

VibrantForests showed much more limited ability to discern the variation in average tree diameter (QMD). The density plot for QMD showed model predictions biased toward the mean with a more peaked distribution and limited extension into the left and right tails compared to the distribution of observed QMD. This pattern was even more pronounced when considering the derived estimate of TPH (data not shown), indicating that VibrantForests is not able to precisely track stem density and diameter across landscapes to the same degree that is apparent for AGB, BA, Height, and Cover predictions.

Summary statistics confirmed limited bias (generally less than 10-15% of mean observed values) and modest correlations between observations and predictions. The amount of variance in field observations explained by model predictions was limited (R²), highlighting the amount of noise and error involved with plot-level comparisons even when reasonable correlations between observations and predictions (Pearson's R) were found.

#### Summary statistics for plot-scale evaluation of VibrantForests 2024 against field plots (2010-2018)
| Target      |   MAE |   RMSE |   Mean Bias |   R² |   Pearson R | Obs (Mean ± Std)   | Pred (Mean ± Std)   |
|:------------|:-----:|:------:|:-----------:|:---------------:|:-----------:|:------------------:|:-------------------:|
| AGB (Mg/ha) | 149.9 |  218.1 |       +37.6 |            0.17 |        0.60 | 270 ± 239          | 307 ± 243           |
| Cover (%)   |  13.4 |   22.0 |        -1.5 |            0.02 |        0.54 | 75 ± 22            | 74 ± 23             |
| Height (m)  |   7.6 |   10.6 |        -0.3 |            0.25 |        0.63 | 29 ± 12            | 29 ± 12             |
| BA (m²/ha)  |  19.9 |   27.4 |        +1.8 |            0.24 |        0.57 | 44 ± 32            | 46 ± 27             |
| QMD (cm)    |  12.8 |   17.7 |        -4.8 |            0.10 |        0.43 | 34 ± 19            | 29 ± 11             |

Some amount of error baked into predictions made by VibrantForests is expected to flow from errors in the underlying allometric model fitted to tabular FIA data and employed to generate the training tiles. When we evaluated how well the training tiles compared to the independent field inventory plots (which were not used to fit the allometric model), we observed slight overprediction biases for AGB, Height, and BA. Although we observed tighter correspondence between QMD for field observations and training tiles than we did for predictions by VibrantForests against these field plots, this correlation was still the weakest among all the target variables.

| !["Error when comparing training tiles against independent field observations"](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/plot_level_evaluation_for_training_tiles.png) |
| :---- |
| *The points in this figure show the values in training tiles ("Predicted") where those tiles covered field inventory plots ("Observed). The black points in each scatter plot show the average value predicted for each quantile of observations, spanning the 5<sup>th</sup> through 95<sup>th</sup> percentiles in steps of 5.* |

### Pixel-scale evaluation
During training, we evaluated validation loss every three epochs using the validation partition. Following the conclusion of model training, we evaluated predictions against tiles in the test partition (the results presented here). For every target variable, the model explained the vast majority of variation in the test tiles with very limited bias, with QMD showing the lowest R² value at 0.78.
| !["Model performance on test tiles."](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/model_performance_on_test_tiles.png) |
| :---- |
| *These 2-D histograms reflect the density of paired observations and predictions with color with high counts in red and low counts in blue. The diagonal dashed red line is the 1:1 line[^2]. The dark red regions near the origin in most of these plots indicate non-forested conditions (with zero AGB, BA, etc.) are often the most common occurrence across landscapes.* |

The differences between the amount of variance explained (R²) for pixel-scale evaluations versus plot-scale evaluations highlight how patterns in ground-truth are much more variable than those represented in the training tiles, and that performance metrics from training tiles should not be misinterpreted as indicators of accuracy or precision in real-world applications.

### Region-scale evaluation
Regional evaluations against FIA observations at the scale of 64,000ha hexagons provided additional evidence of low bias, and revealed limited geographic concentrations of bias that would indicate systematic deficiencies in model learning or the representativeness of diverse regions in the training data. We noticed a modest tendency to overpredict AGB in the Central Valley of southern California and a smaller bias with AGB underprediction along the Appalachian Mountains of the eastern USA. We suspect the underprediction biases observed along the northern borders of Michigan and Minnesota are likely artifacts of hexagons that are dominated by water and that different areal bases are involved in the calculation of hexagon-level averages.

| ![Comparison of VibrantForests AGB vs. Menlove](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/conus_agb_vs_menlove.png) |
| :---- |
| *Comparison of VibrantForests AGB 2024 predictions with Menlove and Healey (2020) summary of FIA observations from 2009-2019.[^3]* |

The pattern observed between hexagon-level averages of observed and predicted AGB was generally consistent with all other target variables. We noticed a tendency for the model to produce Cover and Height averages at regional scales that were modestly lower than the FIA observations. Similar to the plot-scale evaluations, we noticed QMD predictions were relatively less sensitive and precise.

| ![Hexagon-level comparisons for all VibrantForests targets](https://vp-open-science.s3.us-west-2.amazonaws.com/model_cards/assets/VibrantForests/1.1.1/regional_evaluation_hexagon.png) |
| :---- |
| *Hexagon-level means of VibrantForests predictions (2024) and FIA observations (2012-2022).* |

> [!NOTE]  
> The summary statistics in the AGB scatter plot above and in the table below refer only to comparisons of hexagon-level means on an all-area basis. These statistics do not represent a performance evaluation limited to the forested conditions this model was intended for. These statistics are heavily influenced by the inclusion of a large number of non-forest pixels across the landscape, and can give an artificially inflated sense of model performance in forested landscapes solely due to correctly predicting target attributes in non-forested areas as zeros.[^4]  

#### Summary statistics for region-scale evaluation of VibrantForests 2024 against FIA (2012-2022)
|  Target    |   MAE |   RMSE |   Mean Bias |   R² |   Pearson R | Obs (Mean ± SD)   | Pred (Mean ± SD)   |
|:-----------|:-----:|:------:|:-----------:|:---------------:|:-----------:|:-----------------:|:------------------:|
| AGB (Mg/ha)|  15.9 |   26.6 |       -2.37 |            0.78 |        0.90 | 41.6 ± 56.4       | 39.2 ± 42.1        |
| Cover (%)  |   7.1 |   11.5 |       -3.90 |            0.77 |        0.91 | 22.3 ± 24.1       | 18.4 ± 18.0        |
| Height (m) |   2.7 |    4.6 |       -1.85 |            0.76 |        0.93 | 8.4 ± 9.5         | 6.6 ± 6.5          |
| BA (m²/ha) |   2.8 |    4.5 |       -0.54 |            0.77 |        0.89 | 8.0 ± 9.4         | 7.4 ± 7.2          |
| QMD (cm)   |   6.6 |    7.6 |       +5.91 |           -0.13 |        0.76 | 7.4 ± 7.2         | 13.3 ± 4.2         |

## Ethical considerations
The data used and generated by VibrantForests are not considered sensitive nor to pose substantial risks to human health or safety. The data are intended to be instrumental in decision-making that may indirectly enable human health and safety to be better protected through more cost-effective and targeted wildfire and forest restoration planning. 

Increased availability and precision of data on the volume and potential value of forest products and other natural resources such as carbon storage and sequestration may contribute to land speculation. Information asymmetry may contribute to efforts by nefarious actors to capture land values from vulnerable populations with unclear tenure or access to capital that are necessary for these communities to realize these values themselves or protect them from other interested parties. 

Considering a variety of similar data sources already exist at both high and moderate resolution produced by state and federal governments as well as by research groups and the private sector, we do not expect outputs from VibrantForests to create any new or additional risks of harm. 

We are also aware that carbon offset accounting and markets are contentious and that some market observers view these use cases as ethically dubious. We believe these disputes are largely related to the principles and practices of offset accounting and project development and financing, as opposed to the existence or quality of forest structure or biomass maps. We believe that increasing quality of forest structure maps will have overwhelmingly positive effects compared to the potential for negative impacts due to the use of these data by nefarious or unethical actors.

## Caveats and recommendations
VibrantForests has been developed and evaluated with an initial focus on the contiguous USA. We caution against naive application of this model beyond that scope without additional effort to update the training and evaluation datasets to determine model performance in other regions. Similarly, due to the reliance on Sentinel-2 imagery, we have not investigated model performance going back prior to 2018 using coarser resolution data such as Harmonized Landsat and Sentinel imagery. 

In its current form, the model is not conditioned to generate smooth predictions over time. As such, application of the model in a time-series context may generate non-smooth trends that could be problematic for direct estimation of carbon flux or stock change at very high resolution.

### Artifacts related to buildings
During the development of lidar-derived training tiles, the canopy height model and canopy cover statistics were calculated for all extents of processed lidar without eliminating buildings. This resulted in some training tiles where the FIA-derived allometric model was applied to rasters with height and cover attributes that were outside the domain of forest plots and generated some pixels over buildings with outlier values for the estimated forest attributes. When VibrantForests outputs are prepared for downstream uses (e.g., in the Forest Innovation Platform), we mitigated these artifacts by applying masks that zeroed-out predictions where the land cover had been mapped as a built-up type. We utilized a mixture of [Annual NLCD](https://www.usgs.gov/centers/eros/science/annual-nlcd-land-cover-classification), Sentinel-2 based NDVI calculations, along with buildings and roads identified in [Overture Maps](https://overturemaps.org/) to identify built-up pixels. In future iterations of this model, we plan to identify and address buildings in the training data more directly to help the model more cleanly learn to distinguish buildings from vegetation.

### Artifacts related to non-forest conditions
VibrantForests training tiles were generated by applying an FIA-derived allometric model to lidar-derived rasters. The FIA-derived allometric models are applicable to conditions where live trees were observed. We have noticed examples in herbaceous wetlands and flooded agricultural areas that outlier values of forest attributes may be predicted in a way that seems driven by intense greenness and high moisture. When VibrantForests outputs are prepared for downstream uses (e.g., in the Forest Innovation Platform), we mitigated these artifacts by applying masks to zero out predictions of forest attributes over surface water, herbaceous wetlands, bare ground (including perennial snow and ice), and agricultural vegetation, while allowing predictions to pass through unmodified in cover types of shrubs, grasslands, woody wetlands, and all forest cover types.

## Acknowledgements
The development of VibrantForests would not have been possible without the ability to build upon a large collection of lidar data acquired and processed through contributions by many colleauges, including Mitchell Gritts, Zoe Statman-Weil, Ryan Herring, Paige Maas, Bogdan State, Danielle Perrot, Colton Miller, Janet Wiener, and Tory Nelson.

This material is based upon work supported by the U.S. Department of Agriculture, under agreement number NR233A750004G042, and by the USDA Forest Service, under agreement number #24-CA-11132544-064. Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the U.S. Department of Agriculture. In addition, any reference to specific brands or types of products or services does not constitute or imply an endorsement by the U.S. Department of Agriculture for those products or services. 

The development of VibrantForests was also supported by funding from a grant by the Doris Duke Foundation to American Forests for the development of the Forest Innovation Platform.

This model card format was adapted from a Markdown template developed by [Christian Garbin](https://github.com/fau-masters-collected-works-cgarbin/model-card-template), based on sections and prompts from [Mitchell et al. (2019)](https://arxiv.org/abs/1810.03993). 


[^1]: The choice of 10m raster resolution reflects a scale that follows recent scientific recommendations to represent tree biomass maps at no higher a resolution than "the crown diameter of a typical large tree, which ranges from about 10m for temperate forests to about 20m for tropical forests" according to [Duncanson et al. (2025)](https://doi.org/10.1126/science.adt6811) citing crown diameters from [Jucker et al. (2022)](https://doi.org/10.1111/gcb.16302).
[^2]: This figure layout is adapted from similar figures produced in [Kennedy et al. (2020: Figure 5)](https://doi.org/10.1088%2F1748-9326%2Fab93f9) and [Anderson et al. (2025: Figure 5)](https://doi.org/10.32942/X2KW7H) to facilitate comparisons of model learning from training data.
[^3]: This figure layout was adapted from [Anderson et al. (2025: Figure 13)](https://doi.org/10.32942/X2KW7H) to facilitate model comparison for predictive performance against ground-truth.
[^4]: All-area statistics are dominated by a very large number of pixels in non-forest conditions where all forest structure target variables at equal to or near zero. To inform the predictive quality of the model at regional scales within forested conditions, a consistent land cover basis needs to be applied to predictions and FIA observations. Forest-specific evaluations are sigificantly confounded by differences between land cover classifications we employ and those used by the FIA program, which are not released to the public.